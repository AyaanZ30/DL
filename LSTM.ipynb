{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np               \n",
    "import pandas as pd              \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'Name', 'Year', 'Gender', 'Count'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/NationalNames.csv')\n",
    "print(data.columns)\n",
    "\n",
    "data['Name'] = data['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data['Name'][:10000]).reshape(-1, 1)\n",
    "data = [name.lower() for name in data[:, 0]]\n",
    "data = np.array(data).reshape(-1, 1)     # one column having all names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary']\n",
      " ['anna']\n",
      " ['emma']\n",
      " ...\n",
      " ['jens']\n",
      " ['julious']\n",
      " ['lindsay']]\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_name(data):\n",
    "    names_length_vector = [len(name[0]) for name in data]\n",
    "    max_index = names_length_vector.index(max(names_length_vector))\n",
    "    print(\"Longest name:\", data[max_index][0], f'(Length = {max(names_length_vector)})')\n",
    "    return max(names_length_vector)\n",
    "\n",
    "def pad_data(data, longest_name_length):\n",
    "    for index in range(len(data)):\n",
    "        rem_len = (longest_name_length - len(data[index, 0]))\n",
    "        padding_string = '.'*rem_len\n",
    "        data[index, 0] = data[index, 0] + padding_string\n",
    "    return data\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = list()\n",
    "    for name in data[:, 0]:\n",
    "        vocab.extend(list(name))\n",
    "    print(f'Vocab size : {len(set(vocab))}')\n",
    "    print(f'Vocab : {set(vocab)}')\n",
    "    return set(vocab), len(vocab)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest name: francisquita (Length = 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['mary........'],\n",
       "       ['anna........'],\n",
       "       ['emma........']], dtype='<U12')"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data = np.copy(data)\n",
    "longest_name_length = get_longest_name(transformed_data)   \n",
    "\n",
    "transformed_data = pad_data(transformed_data, longest_name_length)\n",
    "transformed_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 27\n",
      "Vocab : {'s', 'y', 'l', '.', 'e', 'p', 'g', 'h', 'd', 'r', 'f', 'n', 'a', 'k', 'q', 'x', 'v', 't', 'z', 'u', 'c', 'b', 'w', 'i', 'o', 'm', 'j'}\n",
      "a-12, 22-w\n"
     ]
    }
   ],
   "source": [
    "vocab, vocab_size = build_vocab(transformed_data)\n",
    "\n",
    "id_char, char_id = dict(), dict()\n",
    "\n",
    "for i, char in enumerate(vocab):\n",
    "    id_char[i] = char \n",
    "    char_id[char] = i  \n",
    "\n",
    "print(f'a-{char_id['a']}, 22-{id_char[22]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches in train data : 500 (20 samples per batch)\n",
      "basically a dataset of 500 samples -> 12 vectors per sample -> 20 ohe vec per vector\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "batch_size = 20 \n",
    "TOTAL_BATCHES = int(len(transformed_data) / batch_size)\n",
    "print(f'Total number of batches in train data : {TOTAL_BATCHES} ({batch_size} samples per batch)')\n",
    "\n",
    "# splitting train data into batches \n",
    "for i in range(len(transformed_data) - batch_size + 1):\n",
    "    start = i * batch_size\n",
    "    end = start + batch_size\n",
    "    \n",
    "    batch_data = transformed_data[start:end]\n",
    "    if(len(batch_data) != batch_size):\n",
    "        break   \n",
    "    \n",
    "    char_list = []\n",
    "    for k in range(len(batch_data[0][0])):\n",
    "        batch_dataset = np.zeros([batch_size, len(vocab)])\n",
    "        for j in range(batch_size):\n",
    "            name = (batch_data[j][0])\n",
    "            char_index = char_id[name[k]]\n",
    "            batch_dataset[j, char_index] = 1.0\n",
    "        \n",
    "        # one-hot-encoding for ith char of each name in batch_data \n",
    "        char_list.append(batch_dataset)\n",
    "    train_dataset.append(char_list)\n",
    "\n",
    "print('basically a dataset of 500 samples -> 12 vectors per sample -> 20 ohe vec per vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams \n",
    "input_units = 100 \n",
    "hidden_units = 256 \n",
    "output_units = vocab_size\n",
    "learning_rate = 5e-3\n",
    "\n",
    "# for adam optimizer calculations\n",
    "beta1 = 0.90\n",
    "beta2 = 0.99\n",
    "epsilon = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "def tanh_activation(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x_sum = np.sum(np.exp(x), axis = 1).reshape(-1, 1)\n",
    "    return (np.exp(x) / exp_x_sum)\n",
    "\n",
    "def tanh_derivative(x): \n",
    "    return (1 - (x ** 2))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return (x * (1 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    params = dict ()\n",
    "    mean, std = 0, 0.01 \n",
    "    \n",
    "    # LSTM memory cell gates weights \n",
    "    input_gate_weights = np.random.normal(mean, std, (input_units + hidden_units, hidden_units))\n",
    "    forget_gate_weights = np.random.normal(mean, std, (input_units + hidden_units, hidden_units))\n",
    "    output_gate_weights = np.random.normal(mean, std, (input_units + hidden_units, hidden_units))\n",
    "    intermediate_gate_weights = np.random.normal(mean, std, (input_units + hidden_units, hidden_units))\n",
    "    \n",
    "    hidden_output_weights = np.random.normal(mean, std, (hidden_units, len(vocab)))\n",
    "    params['fgw'] = forget_gate_weights\n",
    "    params['igw'] = input_gate_weights\n",
    "    params['ogw'] = output_gate_weights \n",
    "    params['ggw'] = intermediate_gate_weights \n",
    "    params['how'] = hidden_output_weights\n",
    "    \n",
    "    return params "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM cell components](LSTM_CELL.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(batch_dataset, prev_hidden_state, prev_cell_state, parameters):\n",
    "    lstm_activations = dict() \n",
    "    \n",
    "    fgw = parameters['fgw']\n",
    "    ogw = parameters['ogw']\n",
    "    igw = parameters['igw']\n",
    "    ggw = parameters['ggw']\n",
    "    \n",
    "    # getting current input (from batch dataset) & prev hidden state \n",
    "    concat_dataset = np.concatenate((batch_dataset, prev_hidden_state), axis = 1)\n",
    "    \n",
    "    fa = sigmoid(np.matmul(concat_dataset, fgw))\n",
    "    ia = sigmoid(np.matmul(concat_dataset, igw))\n",
    "    ga = tanh_activation(np.matmul(concat_dataset, ggw))\n",
    "    oa = sigmoid(np.matmul(concat_dataset, ogw))\n",
    "    \n",
    "    # new cell state \n",
    "    new_cell_state = np.multiply(prev_cell_state, fa) + np.multiply(ia, ga)\n",
    "    # new activation\n",
    "    # new_hidden_state = np.matmul(oa, tanh_activation(new_cell_state))\n",
    "    new_hidden_state = oa * tanh_activation(new_cell_state)\n",
    "    \n",
    "    lstm_activations['fa'] = fa\n",
    "    lstm_activations['ia'] = ia\n",
    "    lstm_activations['ga'] = ga\n",
    "    lstm_activations['oa'] = oa\n",
    "    return lstm_activations, new_cell_state, new_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_cell(current_hidden_state, parameters):\n",
    "    how = parameters['how']\n",
    "    ot = np.matmul(current_hidden_state, how)\n",
    "    return softmax(ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(batch_dataset, embeddings):\n",
    "    embedding_dataset = np.matmul(batch_dataset, embeddings)\n",
    "    return embedding_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Forward propagation](fp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(batches, parameters, embeddings):\n",
    "    lstm_cache = dict()                 # for every cell : (fa, ga, oa, ia)\n",
    "    cell_cache = dict()                 # for every cell : c(t) (cell state)\n",
    "    activation_cache = dict()           # for every cell : a(t)\n",
    "    output_cache = dict()               # for every cell : o(t)\n",
    "    embedding_cache = dict()            # embeddings for each batch : e0, e1,..\n",
    "    \n",
    "    batch_size = batches[0].shape[0]\n",
    "    # initial hidden state(a0) and cell state(c0) \n",
    "    a0 = np.zeros([batch_size, hidden_units], dtype = np.float32)\n",
    "    c0 = np.zeros([batch_size, hidden_units], dtype = np.float32)\n",
    "    \n",
    "    activation_cache['a0'] = a0\n",
    "    cell_cache['c0'] = c0 \n",
    "    \n",
    "    for i in range(len(batches) - 1):\n",
    "        batch_dataset = batches[i]\n",
    "        \n",
    "        # instead of using raw & sparse (mostly 0s) OHE vectors, embedding dim matrix is used to represent better information \n",
    "        batch_dataset = get_embeddings(batch_dataset, embeddings)\n",
    "        embedding_cache['emb'+str(i)] = batch_dataset\n",
    "        \n",
    "        # get activations and new cell state(ct), new hidden state(ht) for current memory cell\n",
    "        lstm_activations, ct, at = lstm_cell(batch_dataset, a0, c0, parameters)\n",
    "        ot = output_cell(at, parameters)\n",
    "        \n",
    "        lstm_cache['lstm'+str(i+1)] = lstm_activations  \n",
    "        output_cache['o'+str(i+1)] = ot\n",
    "        cell_cache['c'+str(i+1)] = ct\n",
    "        activation_cache['a'+str(i+1)] = at  \n",
    "        \n",
    "        a0, c0 = at, ct      # update for next cell \n",
    "    return embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![metrics](metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_accuracy(batch_labels, output_cache):\n",
    "    acc = 0 \n",
    "    loss = 0 \n",
    "    # prob = 1 \n",
    "    batch_size = batch_labels[0].shape[0]\n",
    "    for i in range(1, len(output_cache) + 1):\n",
    "        pred = output_cache['o'+str(i)]\n",
    "        current_batch_labels = batch_labels[i]\n",
    "        \n",
    "        epsilon = 1e-8\n",
    "        loss += np.sum(\n",
    "            (current_batch_labels * np.log(pred + epsilon))\n",
    "            + \n",
    "            ((1-current_batch_labels) * np.log(1-pred + epsilon)),\n",
    "            axis = 1\n",
    "        ).reshape(-1, 1)\n",
    "        acc += np.mean(np.argmax(current_batch_labels, axis = 1) == np.argmax(pred, axis = 1))\n",
    "        \n",
    "    loss = np.sum(loss)*(-1 / batch_size)\n",
    "    acc = acc / len(output_cache)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_cell_error(batch_labels, output_cache, parameters):\n",
    "    output_error_cache = dict()\n",
    "    activation_error_cache = dict()\n",
    "    \n",
    "    for i in range(1, len(output_cache) + 1):\n",
    "        pred = output_cache['o'+str(i)]\n",
    "        labels = batch_labels[i]\n",
    "        \n",
    "        # output error for time step 't'\n",
    "        error_output = (pred - labels)\n",
    "        \n",
    "        how = parameters['how']\n",
    "        # hidden state (activation) error for time step 't'\n",
    "        error_activation = np.matmul(error_output, how.T)\n",
    "        \n",
    "        output_error_cache['eo'+str(i)] = error_output\n",
    "        activation_error_cache['ea'+str(i)] = error_activation\n",
    "    return output_error_cache, activation_error_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_lstm_cell_error(\n",
    "    activation_output_error,\n",
    "    next_activation_error,\n",
    "    next_cell_error,\n",
    "    parameters,\n",
    "    lstm_activation,\n",
    "    cell_activation,\n",
    "    prev_cell_activation\n",
    "):\n",
    "    if next_activation_error.shape[1] != activation_output_error.shape[1]:\n",
    "        # Pad next_activation_error to match activation_output_error shape\n",
    "        next_activation_error = np.pad(\n",
    "            next_activation_error,\n",
    "            ((0, 0), (0, activation_output_error.shape[1] - next_activation_error.shape[1])),\n",
    "            'constant'\n",
    "        )\n",
    "    # error of hidden state h(t) through output gate\n",
    "    activation_error = activation_output_error + next_activation_error  \n",
    "    \n",
    "    # output gate error (oa) (while bptt) \n",
    "    oa = lstm_activation['oa']\n",
    "    eo = activation_error * tanh_activation(cell_activation) * sigmoid_derivative(oa)\n",
    "    \n",
    "    # cell activation error (c(t) error)\n",
    "    cell_error = (activation_error * oa * tanh_derivative(tanh_activation(cell_activation)))\n",
    "    cell_error += next_cell_error   # accumulating next cell's error as well in bptt\n",
    "    \n",
    "    # input gate error (ia & ga) (while bptt) \n",
    "    ia = lstm_activation['ia']\n",
    "    ga = lstm_activation['ga']\n",
    "    ei = (cell_error * ga * sigmoid_derivative(ia))\n",
    "    \n",
    "    # intermediate gate (ga) error (while bptt)\n",
    "    eg = cell_error * ia * tanh_derivative(ga)\n",
    "    \n",
    "    # forget gate error (fa) (while bptt)\n",
    "    # prev_cell_activation : activation value that forget gate receiv   ed from prev cell (and decided to retain/discard)\n",
    "    fa = lstm_activation['fa']\n",
    "    ef = cell_error * prev_cell_activation * sigmoid_derivative(fa)\n",
    "    \n",
    "    # Error to propagate to previous time step's cell state c(t-1)\n",
    "    prev_cell_error = np.multiply(cell_error, fa)\n",
    "    \n",
    "    # getting weights of gates from parameters \n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ggw = parameters['ggw']\n",
    "    ogw = parameters['ogw']\n",
    "    \n",
    "    # embedding + hidden activation error \n",
    "    embed_activation_error = ef@fgw.T + ei@igw.T + eo@ogw.T + eg@ggw.T \n",
    "    \n",
    "    input_hidden_units = fgw.shape[0]\n",
    "    hidden_units = fgw.shape[1]\n",
    "    input_units = input_hidden_units - hidden_units\n",
    "    \n",
    "    # prev activation error (Splits error to get portion for previous hidden state h(t-1) & x(t) as embedding error)\n",
    "    prev_activation_error = embed_activation_error[:, hidden_units:]\n",
    "    # current input error (x(t))\n",
    "    embed_error = embed_activation_error[:, :input_units]\n",
    "    \n",
    "    lstm_error = dict()\n",
    "    lstm_error['ef'] = ef \n",
    "    lstm_error['ei'] = ei  \n",
    "    lstm_error['eg'] = eg \n",
    "    lstm_error['eo'] = eo   \n",
    "    \n",
    "    return prev_activation_error, prev_cell_error, embed_error, lstm_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_cell_derivatives(output_error_cache, activation_cache, parameters):\n",
    "    derivative_hidden_output_weights = np.zeros_like(parameters['how'])\n",
    "    \n",
    "    # the first activation after processing the first batch would be a1 (as a0 would be intiial start)\n",
    "    batch_size = activation_cache['a1'].shape[0]\n",
    "    for i in range(1, len(output_error_cache) + 1):\n",
    "        output_error = output_error_cache['eo'+str(i)]   \n",
    "        activation = activation_cache['a'+str(i)]\n",
    "        \n",
    "        # dhow = (t=1 -> t=T)Î£ (activation(t).T * output_error(t)) / batch_size\n",
    "        derivative_hidden_output_weights += np.matmul(activation.T, output_error) / batch_size\n",
    "    return derivative_hidden_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_lstm_cell_derivatives(lstm_error, activation_matrix, embedding_matrix):\n",
    "    # get all errors of LSTM cell for a single time step from cache\n",
    "    ei = lstm_error['ei']\n",
    "    ef = lstm_error['ef']\n",
    "    eo = lstm_error['eo']\n",
    "    eg = lstm_error['eg']\n",
    "    \n",
    "    # get input activations for this time step\n",
    "    concat_matrix = np.concatenate((activation_matrix, embedding_matrix), axis = 1)\n",
    "    batch_size = embedding_matrix.shape[0]\n",
    "    \n",
    "    dfgw = np.matmul(concat_matrix.T, ef) / batch_size\n",
    "    digw = np.matmul(concat_matrix.T, ei) / batch_size\n",
    "    dogw = np.matmul(concat_matrix.T, eo) / batch_size\n",
    "    dggw = np.matmul(concat_matrix.T, eg) / batch_size\n",
    "    \n",
    "    derivatives = dict()\n",
    "    derivatives['dfgw'] = dfgw  \n",
    "    derivatives['digw'] = digw \n",
    "    derivatives['dogw'] = dogw \n",
    "    derivatives['dggw'] = dggw\n",
    "    \n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to compute gradients for all weights using the chain rule considering LSTM-specific gates and memory cells.\n",
    "def backward_propagation(\n",
    "    parameters, \n",
    "    batch_labels,\n",
    "    embedding_cache, \n",
    "    lstm_cache, \n",
    "    activation_cache, \n",
    "    cell_cache, \n",
    "    output_cache\n",
    "):\n",
    "    output_errors_cache, activation_errors_cache = output_cell_error(batch_labels, output_cache, parameters)\n",
    "    # to store lstm error for each time step\n",
    "    lstm_error_cache = dict()\n",
    "    # to store embeddings (input) error for each time step\n",
    "    embedding_error_cache = dict()\n",
    "    \n",
    "    # At the last time step (t=T), there are no future errors to propagate    \n",
    "    error_activation_at_t = np.zeros(activation_cache['a1'].shape)\n",
    "    error_cellstate_at_t = np.zeros(cell_cache['c1'].shape)\n",
    "    \n",
    "    # loop in reverse order (from t = T -> t = 0)\n",
    "    for i in range(len(lstm_cache), 0, -1): \n",
    "        prev_activation_error, prev_cell_error, embed_error, lstm_error = single_lstm_cell_error(\n",
    "            activation_output_error = activation_cache['a'+str(i)],\n",
    "            next_activation_error = error_activation_at_t,\n",
    "            next_cell_error = error_cellstate_at_t, \n",
    "            parameters = parameters, \n",
    "            lstm_activation = lstm_cache['lstm'+str(i)],  \n",
    "            cell_activation = cell_cache['c'+str(i)], \n",
    "            prev_cell_activation = cell_cache['c'+str(i-1)]\n",
    "        )\n",
    "        lstm_error_cache['elstm'+str(i)] = lstm_error \n",
    "        embedding_error_cache['eemb'+str(i)] = embed_error\n",
    "        \n",
    "        # now we go for next (t-1) so prev would be at t\n",
    "        error_activation_at_t = prev_activation_error\n",
    "        error_cellstate_at_t = prev_cell_error\n",
    "    \n",
    "    # calculating output cell derivative (gradient) \n",
    "    derivatives = dict()\n",
    "    # derivative_hidden_output_weights -> derivatives ['dhow']\n",
    "    derivatives['dhow'] = output_cell_derivatives(\n",
    "        output_error_cache = output_errors_cache, \n",
    "        activation_cache = activation_cache,\n",
    "        parameters = parameters \n",
    "    )\n",
    "    \n",
    "    # caiculatign lstm cell derivatives at each time step and storing in dict\n",
    "    lstm_derivatives = dict()\n",
    "    for i in range(1, len(lstm_error_cache) + 1):\n",
    "        lstm_derivatives['dlstm'+str(i)] = single_lstm_cell_derivatives(\n",
    "            lstm_error = lstm_error_cache['elstm'+str(i)], \n",
    "            activation_matrix = activation_cache['a'+str(i-1)], \n",
    "            embedding_matrix = embedding_cache['emb'+str(i-1)]\n",
    "        )\n",
    "        \n",
    "    #initialize the derivatives to zeros \n",
    "    derivatives['dfgw'] = np.zeros(parameters['fgw'].shape)\n",
    "    derivatives['digw'] = np.zeros(parameters['igw'].shape)\n",
    "    derivatives['dogw'] = np.zeros(parameters['ogw'].shape)\n",
    "    derivatives['dggw'] = np.zeros(parameters['ggw'].shape)\n",
    "    \n",
    "    # sum up derivatives for each time step (all 4 gates weights independently for each lstm cell)\n",
    "    for i in range(1, len(lstm_error_cache) + 1):\n",
    "        derivatives['dfgw'] += lstm_derivatives['dlstm'+str(i)]['dfgw']\n",
    "        derivatives['digw'] += lstm_derivatives['dlstm'+str(i)]['digw']\n",
    "        derivatives['dogw'] += lstm_derivatives['dlstm'+str(i)]['dogw']\n",
    "        derivatives['dggw'] += lstm_derivatives['dlstm'+str(i)]['dggw']\n",
    "    \n",
    "    return derivatives, embedding_error_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam optimizer <= Momentum based optimization + RMSProp optimization\n",
    "\n",
    "![Adam](https://i.sstatic.net/GyMqA.png)\n",
    "\n",
    "(1e-7 : epsilon to avoid division by zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params_adam_optimization(parameters, derivatives, V, S):\n",
    "    # get the derivatives\n",
    "    dfgw = derivatives['dfgw']\n",
    "    digw = derivatives['digw']\n",
    "    dogw = derivatives['dogw']\n",
    "    dggw = derivatives['dggw']\n",
    "    dhow = derivatives['dhow']\n",
    "    \n",
    "    # get parameters \n",
    "    fgw = parameters['fgw']\n",
    "    igw = parameters['igw']\n",
    "    ogw = parameters['ogw']\n",
    "    ggw = parameters['ggw']\n",
    "    how = parameters['how']\n",
    "    \n",
    "    # get V (MOMENTUM) parameters (as learning rate is to be optimized for each parameter)\n",
    "    vfgw = V['vfgw']\n",
    "    vigw = V['vigw']\n",
    "    vogw = V['vogw']\n",
    "    vggw = V['vggw']\n",
    "    vhow = V['vhow']\n",
    "    \n",
    "    # get S (RMS-PROP) parameters (as learning rate is to be optimized for each parameter)\n",
    "    sfgw = S['sfgw']\n",
    "    sigw = S['sigw']\n",
    "    sogw = S['sogw']\n",
    "    sggw = S['sggw']\n",
    "    show = S['show']\n",
    "    \n",
    "    # calculate momentum aspect for the moving of gradients\n",
    "    vfgw = (beta1 * vfgw) + (1-beta1)*(dfgw)\n",
    "    vigw = (beta1 * vigw) + (1-beta1)*(digw)\n",
    "    vogw = (beta1 * vogw) + (1-beta1)*(dogw)\n",
    "    vggw = (beta1 * vggw) + (1-beta1)*(dggw)\n",
    "    vhow = (beta1 * vhow) + (1-beta1)*(dhow)\n",
    "    \n",
    "    # calculate the RMS aspect using square of gradients\n",
    "    sfgw = (beta2 * sfgw) + (1-beta2)*(dfgw**2)\n",
    "    sigw = (beta2 * sigw) + (1-beta2)*(digw**2)\n",
    "    sogw = (beta2 * sogw) + (1-beta2)*(dogw**2)\n",
    "    sggw = (beta2 * sggw) + (1-beta2)*(dggw**2)\n",
    "    show = (beta2 * show) + (1-beta2)*(dhow**2)\n",
    "    \n",
    "    # FINALLY, UPDATE WEIGHTS USING FORMULA IN THE PICTURE\n",
    "    fgw = fgw - (learning_rate)*((vfgw)/np.sqrt(sfgw) + epsilon)\n",
    "    igw = igw - (learning_rate)*((vigw)/np.sqrt(sigw) + epsilon)\n",
    "    ogw = ogw - (learning_rate)*((vogw)/np.sqrt(sogw) + epsilon)\n",
    "    ggw = ggw - (learning_rate)*((vggw)/np.sqrt(sggw) + epsilon)\n",
    "    how = how - (learning_rate)*((vhow)/np.sqrt(show) + epsilon)\n",
    "    \n",
    "    # store the updated weights by replacing initial ones\n",
    "    parameters['fgw'] = fgw \n",
    "    parameters['igw'] = igw \n",
    "    parameters['ogw'] = ogw \n",
    "    parameters['ggw'] = ggw \n",
    "    parameters['how'] = how \n",
    "    \n",
    "    # store updated V params    \n",
    "    V['vfgw'] = vfgw\n",
    "    V['vigw'] = vigw \n",
    "    V['vogw'] = vogw \n",
    "    V['vggw'] = vggw\n",
    "    V['vhow'] = vhow\n",
    "    \n",
    "    #store updated S parameters\n",
    "    S['sfgw'] = sfgw \n",
    "    S['sigw'] = sigw \n",
    "    S['sogw'] = sogw \n",
    "    S['sggw'] = sggw\n",
    "    S['show'] = show\n",
    "    \n",
    "    return parameters, V, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_V(parameters):\n",
    "    V = dict()\n",
    "    V['vfgw'] = np.zeros(parameters['fgw'].shape)\n",
    "    V['vigw'] = np.zeros(parameters['igw'].shape)\n",
    "    V['vogw'] = np.zeros(parameters['ogw'].shape)\n",
    "    V['vggw'] = np.zeros(parameters['ggw'].shape)\n",
    "    V['vhow'] = np.zeros((hidden_units, len(vocab)))\n",
    "    return V\n",
    "\n",
    "def initialize_S(parameters):\n",
    "    S = dict()\n",
    "    S['sfgw'] = np.zeros(parameters['fgw'].shape)\n",
    "    S['sigw'] = np.zeros(parameters['igw'].shape)\n",
    "    S['sogw'] = np.zeros(parameters['ogw'].shape)\n",
    "    S['sggw'] = np.zeros(parameters['ggw'].shape)\n",
    "    S['show'] = np.zeros((hidden_units, len(vocab)))  # Use vocab size\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new embeddings = old embeddings - (leanring rate) * (embedding_derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(embeddings, embedding_error_cache, batch_labels):\n",
    "    embedding_derivatives = np.zeros(embeddings.shape)\n",
    "    batch_size = batch_labels[0].shape[0]\n",
    "    \n",
    "    for i in range(len(embedding_error_cache)):\n",
    "        embedding_derivatives += np.matmul(\n",
    "            batch_labels[i].T, \n",
    "            embedding_error_cache['eemb'+str(i+1)]\n",
    "        ) / (batch_size)\n",
    "    \n",
    "    embeddings = embeddings - (learning_rate) * (embedding_derivatives)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training LSTM steps:\n",
    "\n",
    "Initialize Parameters -->\n",
    "Forward Propagation -->\n",
    "Calculate Loss, Perplexity, acc -->\n",
    "Backward Propagation -->\n",
    "Update weights & embeddings (ADAM optimization) --> Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(train_dataset, iterations = 8000, batch_size = 20):\n",
    "    print('Train dataset (no of batchesm) : ',len(train_dataset))\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    V = initialize_V(parameters)\n",
    "    S = initialize_S(parameters)\n",
    "    \n",
    "    # initialize embeddings of shape (27 x 100) => (vocab size x input units)\n",
    "    embeddings = np.random.normal(0, 0.01, (len(vocab), input_units))\n",
    "    J, A = [], []              # loss, perplexity, accuracy\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        index = i % len(train_dataset)\n",
    "        batches = train_dataset[index]\n",
    "        \n",
    "        # FP\n",
    "        embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache = forward_propagation(\n",
    "            batches = batches,\n",
    "            parameters = parameters, \n",
    "            embeddings = embeddings\n",
    "        )\n",
    "        \n",
    "        # Metrics\n",
    "        # perplexity, loss, accuracy = calculate_loss_accuracy(batches, output_cache)\n",
    "        loss, accuracy = calculate_loss_accuracy(batches, output_cache)\n",
    "        \n",
    "        # BPTT \n",
    "        derivatives, embedding_error_cache = backward_propagation(\n",
    "            parameters = parameters, \n",
    "            batch_labels = batches, \n",
    "            embedding_cache = embedding_cache,\n",
    "            lstm_cache = lstm_cache,\n",
    "            activation_cache = activation_cache, \n",
    "            cell_cache = cell_cache, \n",
    "            output_cache = output_cache  \n",
    "        )\n",
    "        \n",
    "        # updating params using adam\n",
    "        parameters, V, S = update_params_adam_optimization(\n",
    "            parameters = parameters, \n",
    "            derivatives = derivatives,\n",
    "            V = V, \n",
    "            S = S\n",
    "        )\n",
    "        \n",
    "         # updating embeddings\n",
    "        embeddings = update_embeddings(\n",
    "            embeddings = embeddings,\n",
    "            embedding_error_cache = embedding_error_cache, \n",
    "            batch_labels = batches\n",
    "        )\n",
    "        \n",
    "        J.append(loss)\n",
    "        A.append(accuracy)\n",
    "        \n",
    "        if(i%1000==0):\n",
    "            print(\"For Single Batch :\")\n",
    "            print('Step       = {}'.format(i))\n",
    "            print('Loss       = {}'.format(round(loss,2)))\n",
    "            # print('Perplexity = {}'.format(round(perplexity,2)))\n",
    "            print('Accuracy   = {}'.format(round(accuracy*100,2)))\n",
    "            print()\n",
    "        \n",
    "    return embeddings, parameters, J, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset (no of batchesm) :  500\n",
      "For Single Batch :\n",
      "Step       = 0\n",
      "Loss       = 47.05\n",
      "Accuracy   = 0.45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91932\\AppData\\Local\\Temp\\ipykernel_14648\\3106524436.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Single Batch :\n",
      "Step       = 1000\n",
      "Loss       = 42.95\n",
      "Accuracy   = 62.27\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 2000\n",
      "Loss       = 44.85\n",
      "Accuracy   = 61.82\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 3000\n",
      "Loss       = 44.61\n",
      "Accuracy   = 10.45\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 4000\n",
      "Loss       = 46.26\n",
      "Accuracy   = 10.45\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 5000\n",
      "Loss       = 46.08\n",
      "Accuracy   = 4.55\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 6000\n",
      "Loss       = 46.4\n",
      "Accuracy   = 55.91\n",
      "\n",
      "For Single Batch :\n",
      "Step       = 7000\n",
      "Loss       = 46.43\n",
      "Accuracy   = 40.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings, parameters, J, A = train_lstm(train_dataset = train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, embeddings, id_char, vocab_size):\n",
    "    names = []\n",
    "    for i in range(20):\n",
    "        name = ''\n",
    "        a0 = np.zeros([1, hidden_units], dtype = np.float32)\n",
    "        c0 = np.zeros([1, hidden_units], dtype = np.float32)\n",
    "        \n",
    "        batch_dataset = np.zeros([1, len(vocab)])   # get random start character(to feed to LSTM seq)\n",
    "        index = np.random.randint(0, 27, 1)[0]  \n",
    "        batch_dataset[0, index] = 1.0            # randomly set a sparse value (0) to 1 among 27 values (shape : 1, 27)\n",
    "        name += id_char[index]\n",
    "        char = id_char[index]\n",
    "        \n",
    "        while char != '.':\n",
    "            batch_dataset = get_embeddings(batch_dataset, embeddings)\n",
    "            lstm_activations, ct, at = lstm_cell(batch_dataset, a0, c0, parameters)\n",
    "            ot = output_cell(at, parameters)\n",
    "            \n",
    "            # probab sampling (instead of argmax) -> for better creativity in generating names\n",
    "            pred = np.random.choice(27, 1, p = ot[0])[0]\n",
    "            \n",
    "            name += id_char[pred]\n",
    "            char += id_char[pred]\n",
    "            \n",
    "            # update input for next prediction in sequence\n",
    "            batch_dataset = np.zeros([1, len(vocab)])\n",
    "            batch_dataset[0, pred] = 1.0\n",
    "            \n",
    "            #update a0 and c0 to new 'at' and 'ct' for next lstm cell\n",
    "            a0 = at \n",
    "            c0 = ct\n",
    "        names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91932\\AppData\\Local\\Temp\\ipykernel_14648\\3106524436.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[410], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_char\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mid_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[409], line 20\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(parameters, embeddings, id_char, vocab_size)\u001b[0m\n\u001b[0;32m     17\u001b[0m ot \u001b[38;5;241m=\u001b[39m output_cell(at, parameters)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# probab sampling (instead of argmax) -> for better creativity in generating names\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mot\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     22\u001b[0m name \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m id_char[pred]\n\u001b[0;32m     23\u001b[0m char \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m id_char[pred]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predict(parameters = parameters, embeddings = embeddings, id_char = id_char, vocab_size = len(vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
