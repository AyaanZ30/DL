{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_net_value(w1, x1, w2, x2, bias):\n",
    "    \"\"\"Calculates the weighted sum for a neuron.\"\"\"\n",
    "    net = (w1 * x1) + (w2 * x2) + bias\n",
    "    return net\n",
    "\n",
    "def sigmoid_activation(weighted_sum):\n",
    "    \"\"\"Applies the sigmoid activation function.\"\"\"\n",
    "    power = -1 * weighted_sum\n",
    "    layer_output = 1.0 / (1.0 + math.exp(power))\n",
    "    return layer_output\n",
    "\n",
    "def sigmoid_derivative(output_val): \n",
    "    \"\"\"Calculates the derivative of the sigmoid function.\"\"\"\n",
    "    return output_val * (1 - output_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_error_derivative(actual_output, target_output):\n",
    "    \"\"\"Calculates the derivative of the total error with respect to an actual output.\"\"\"\n",
    "    # Note: This is dE_total / d(actual_output_neuron)\n",
    "    # If E_total = 0.5 * (target1 - actual1)^2 + 0.5 * (target2 - actual2)^2\n",
    "    # Then dE_total / d(actual1) = actual1 - target1\n",
    "    return actual_output - target_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_individual_error(target_output, actual_output):\n",
    "    \"\"\"Calculates the squared error for a single output neuron.\"\"\"\n",
    "    # Using math.pow for direct translation, np.power can also be used\n",
    "    error = 0.5 * math.pow((target_output - actual_output), 2)\n",
    "    return error\n",
    "\n",
    "def evaluate_total_error(error_1, error_2):\n",
    "    \"\"\"Calculates the sum of individual errors.\"\"\"\n",
    "    return error_1 + error_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiate_x_power_n(x, n): # This function isn't used in the main C++ logic, but translated\n",
    "    \"\"\"Calculates the derivative of x^n.\"\"\"\n",
    "    return float(n * math.pow(x, (n - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hidden_layer_output(i1, i2, w1, w2, w3, w4, b1):\n",
    "    \"\"\"Calculates the outputs of the hidden layer neurons.\n",
    "    Returns:\n",
    "        tuple: (output_h1, output_h2)\n",
    "    \"\"\"\n",
    "    net_h1 = calculate_net_value(w1, i1, w3, i2, b1)\n",
    "    output_h1 = sigmoid_activation(net_h1)\n",
    "\n",
    "    net_h2 = calculate_net_value(w2, i1, w4, i2, b1) # Note: w2 and w4 are for h2, not w3 for h2\n",
    "    output_h2 = sigmoid_activation(net_h2)\n",
    "    return output_h1, output_h2\n",
    "\n",
    "def calculate_final_layer_output(h1, h2, w5, w6, w7, w8, b2):\n",
    "    \"\"\"Calculates the outputs of the final output layer neurons.\n",
    "    Returns:\n",
    "        tuple: (output_o1, output_o2)\n",
    "    \"\"\"\n",
    "    net_o1 = calculate_net_value(w5, h1, w6, h2, b2) # w5, w6 for o1\n",
    "    output_o1 = sigmoid_activation(net_o1)\n",
    "\n",
    "    net_o2 = calculate_net_value(w7, h1, w8, h2, b2) # w7, w8 for o2\n",
    "    output_o2 = sigmoid_activation(net_o2)\n",
    "    return output_o1, output_o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_output_layer_gradients(target_o1, target_o2, output_o1, output_o2,\n",
    "                                     output_h1, output_h2):\n",
    "    \"\"\"Calculates gradients for the weights connecting hidden layer to output layer.\n",
    "    Returns:\n",
    "        tuple: (dE_total_dw5, dE_total_dw6, dE_total_dw7, dE_total_dw8)\n",
    "    \"\"\"\n",
    "    # For w5 and w6 (affecting output_o1)\n",
    "    dE_total_doutput_o1 = total_error_derivative(output_o1, target_o1)\n",
    "    doutput_o1_dsum_o1 = sigmoid_derivative(output_o1)\n",
    "    dsum_o1_dw5 = output_h1  # d(net_o1)/dw5\n",
    "    dsum_o1_dw6 = output_h2  # d(net_o1)/dw6\n",
    "\n",
    "    dE_total_dw5 = dE_total_doutput_o1 * doutput_o1_dsum_o1 * dsum_o1_dw5\n",
    "    dE_total_dw6 = dE_total_doutput_o1 * doutput_o1_dsum_o1 * dsum_o1_dw6\n",
    "\n",
    "    # For w7 and w8 (affecting output_o2)\n",
    "    dE_total_doutput_o2 = total_error_derivative(output_o2, target_o2)\n",
    "    doutput_o2_dsum_o2 = sigmoid_derivative(output_o2)\n",
    "    dsum_o2_dw7 = output_h1  # d(net_o2)/dw7\n",
    "    dsum_o2_dw8 = output_h2  # d(net_o2)/dw8\n",
    "\n",
    "    dE_total_dw7 = dE_total_doutput_o2 * doutput_o2_dsum_o2 * dsum_o2_dw7\n",
    "    dE_total_dw8 = dE_total_doutput_o2 * doutput_o2_dsum_o2 * dsum_o2_dw8\n",
    "\n",
    "    return dE_total_dw5, dE_total_dw6, dE_total_dw7, dE_total_dw8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hidden_layer_gradients(i1, i2, output_h1, output_h2, w5, w6, w7, w8,\n",
    "                                     dE_total_doutput_o1, doutput_o1_dsum_o1,\n",
    "                                     dE_total_doutput_o2, doutput_o2_dsum_o2):\n",
    "    \"\"\"Calculates gradients for the weights connecting input layer to hidden layer.\n",
    "    Returns:\n",
    "        tuple: (dE_total_dw1, dE_total_dw2, dE_total_dw3, dE_total_dw4)\n",
    "    \"\"\"\n",
    "    # For w1 and w3 (affecting output_h1)\n",
    "    # dE_total / d(output_h1)\n",
    "    dE_total_doutput_h1 = (dE_total_doutput_o1 * doutput_o1_dsum_o1 * w5) + \\\n",
    "                          (dE_total_doutput_o2 * doutput_o2_dsum_o2 * w7)\n",
    "    doutput_h1_dsum_h1 = sigmoid_derivative(output_h1)\n",
    "    dsum_h1_dw1 = i1  # d(net_h1)/dw1\n",
    "    dsum_h1_dw3 = i2  # d(net_h1)/dw3 (Corrected from dw2 in C++ comment)\n",
    "\n",
    "    dE_total_dw1 = dE_total_doutput_h1 * doutput_h1_dsum_h1 * dsum_h1_dw1\n",
    "    # The C++ code had dE_total_dw2 associated with dsum_h1_dw2 = i2\n",
    "    # Based on how w1,w2,w3,w4 are used for net_h1 and net_h2:\n",
    "    # net_h1 = w1*i1 + w3*i2 + b1 --> depends on w1 and w3\n",
    "    # net_h2 = w2*i1 + w4*i2 + b1 --> depends on w2 and w4\n",
    "    # So, the gradient for w3 should use dE_total_doutput_h1\n",
    "    dE_total_dw3 = dE_total_doutput_h1 * doutput_h1_dsum_h1 * dsum_h1_dw3\n",
    "\n",
    "\n",
    "    # For w2 and w4 (affecting output_h2)\n",
    "    # dE_total / d(output_h2)\n",
    "    dE_total_doutput_h2 = (dE_total_doutput_o1 * doutput_o1_dsum_o1 * w6) + \\\n",
    "                          (dE_total_doutput_o2 * doutput_o2_dsum_o2 * w8)\n",
    "    doutput_h2_dsum_h2 = sigmoid_derivative(output_h2)\n",
    "    dsum_h2_dw2 = i1  # d(net_h2)/dw2 (Corrected from dw3 in C++ comment)\n",
    "    dsum_h2_dw4 = i2  # d(net_h2)/dw4\n",
    "\n",
    "    dE_total_dw2 = dE_total_doutput_h2 * doutput_h2_dsum_h2 * dsum_h2_dw2\n",
    "    dE_total_dw4 = dE_total_doutput_h2 * doutput_h2_dsum_h2 * dsum_h2_dw4\n",
    "\n",
    "    return dE_total_dw1, dE_total_dw2, dE_total_dw3, dE_total_dw4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(learning_rate, w1, w2, w3, w4, w5, w6, w7, w8,\n",
    "                   dE_total_dw1, dE_total_dw2, dE_total_dw3, dE_total_dw4,\n",
    "                   dE_total_dw5, dE_total_dw6, dE_total_dw7, dE_total_dw8):\n",
    "    \"\"\"Updates the weights using gradient descent.\n",
    "    Returns:\n",
    "        tuple: Updated weights (w1, w2, ..., w8)\n",
    "    \"\"\"\n",
    "    w1 -= (learning_rate * dE_total_dw1)\n",
    "    w2 -= (learning_rate * dE_total_dw2)\n",
    "    w3 -= (learning_rate * dE_total_dw3)\n",
    "    w4 -= (learning_rate * dE_total_dw4)\n",
    "    w5 -= (learning_rate * dE_total_dw5)\n",
    "    w6 -= (learning_rate * dE_total_dw6)\n",
    "    w7 -= (learning_rate * dE_total_dw7)\n",
    "    w8 -= (learning_rate * dE_total_dw8)\n",
    "    return w1, w2, w3, w4, w5, w6, w7, w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize variables\n",
    "    w1, w2, w3, w4 = 0.1, 0.2, 0.3, 0.4\n",
    "    w5, w6, w7, w8 = 0.5, 0.6, 0.7, 0.8\n",
    "    b1, b2 = 0.25, 0.35 # Biases\n",
    "\n",
    "    i1, i2 = 0.1, 0.5 # Inputs\n",
    "    target_o1, target_o2 = 0.05, 0.95 # Target outputs\n",
    "    learning_rate = 0.6\n",
    "\n",
    "    # --- Forward Pass ---\n",
    "    # Calculate hidden layer outputs\n",
    "    output_h1, output_h2 = calculate_hidden_layer_output(i1, i2, w1, w2, w3, w4, b1)\n",
    "    print(f\"Hidden layer outputs: h1={output_h1:.4f}, h2={output_h2:.4f}\")\n",
    "\n",
    "    # Calculate final layer outputs\n",
    "    output_o1, output_o2 = calculate_final_layer_output(output_h1, output_h2, w5, w6, w7, w8, b2)\n",
    "    print(f\"Final layer outputs: o1={output_o1:.4f}, o2={output_o2:.4f}\")\n",
    "\n",
    "    # --- Calculate Error ---\n",
    "    E1 = evaluate_individual_error(target_o1, output_o1)\n",
    "    E2 = evaluate_individual_error(target_o2, output_o2)\n",
    "    E_total = evaluate_total_error(E1, E2)\n",
    "    print(f\"Total Error: E_total={E_total:.4f}\")\n",
    "\n",
    "    # --- Backward Pass ---\n",
    "    # Calculate gradients for the output layer weights (w5, w6, w7, w8)\n",
    "    dE_total_dw5, dE_total_dw6, dE_total_dw7, dE_total_dw8 = \\\n",
    "        calculate_output_layer_gradients(target_o1, target_o2, output_o1, output_o2,\n",
    "                                         output_h1, output_h2)\n",
    "\n",
    "    # Need these terms for hidden layer gradient calculation\n",
    "    # Derivative of total error w.r.t. output of neuron o1\n",
    "    dE_total_doutput_o1 = total_error_derivative(output_o1, target_o1)\n",
    "    # Derivative of output of neuron o1 w.r.t. its net input\n",
    "    doutput_o1_dsum_o1 = sigmoid_derivative(output_o1)\n",
    "\n",
    "    # Derivative of total error w.r.t. output of neuron o2\n",
    "    dE_total_doutput_o2 = total_error_derivative(output_o2, target_o2)\n",
    "    # Derivative of output of neuron o2 w.r.t. its net input\n",
    "    doutput_o2_dsum_o2 = sigmoid_derivative(output_o2)\n",
    "\n",
    "    # Calculate gradients for the hidden layer weights (w1, w2, w3, w4)\n",
    "    dE_total_dw1, dE_total_dw2, dE_total_dw3, dE_total_dw4 = \\\n",
    "        calculate_hidden_layer_gradients(i1, i2, output_h1, output_h2, w5, w6, w7, w8,\n",
    "                                         dE_total_doutput_o1, doutput_o1_dsum_o1,\n",
    "                                         dE_total_doutput_o2, doutput_o2_dsum_o2)\n",
    "\n",
    "    # --- Update Weights ---\n",
    "    w1, w2, w3, w4, w5, w6, w7, w8 = \\\n",
    "        update_weights(learning_rate, w1, w2, w3, w4, w5, w6, w7, w8,\n",
    "                       dE_total_dw1, dE_total_dw2, dE_total_dw3, dE_total_dw4,\n",
    "                       dE_total_dw5, dE_total_dw6, dE_total_dw7, dE_total_dw8)\n",
    "\n",
    "    print(\"\\nNew weights after update:\")\n",
    "    print(f\"w1: {w1:.4f}\")\n",
    "    print(f\"w2: {w2:.4f}\")\n",
    "    print(f\"w3: {w3:.4f}\")\n",
    "    print(f\"w4: {w4:.4f}\")\n",
    "    print(f\"w5: {w5:.4f}\")\n",
    "    print(f\"w6: {w6:.4f}\")\n",
    "    print(f\"w7: {w7:.4f}\")\n",
    "    print(f\"w8: {w8:.4f}\")\n",
    "    \n",
    "    \n",
    "       # --- Optional: Forward pass with new weights to see error reduction ---\n",
    "    output_h1_new, output_h2_new = calculate_hidden_layer_output(i1, i2, w1, w2, w3, w4, b1)\n",
    "    output_o1_new, output_o2_new = calculate_final_layer_output(output_h1_new, output_h2_new, w5, w6, w7, w8, b2)\n",
    "    E1_new = evaluate_individual_error(target_o1, output_o1_new)\n",
    "    E2_new = evaluate_individual_error(target_o2, output_o2_new)\n",
    "    E_total_new = evaluate_total_error(E1_new, E2_new)\n",
    "    print(f\"\\nError after one update: E_total_new={E_total_new:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer outputs: h1=0.6011, h2=0.6154\n",
      "Final layer outputs: o1=0.7349, o2=0.7796\n",
      "Total Error: E_total=0.2491\n",
      "\n",
      "New weights after update:\n",
      "w1: 0.0993\n",
      "w2: 0.1992\n",
      "w3: 0.2967\n",
      "w4: 0.3960\n",
      "w5: 0.4519\n",
      "w6: 0.5507\n",
      "w7: 0.7106\n",
      "w8: 0.8108\n",
      "\n",
      "Error after one update: E_total_new=0.2407\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
